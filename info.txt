Algorithm used in it is Nearest Neighbours

recommedation system has 3 types - content based, collaberative filtering based, hybrid

a)content based- recommend on basis on content similarity eg.if u listening some pop music then the system will 
further recommend pop musics basis on which u listen 
-based on tags which we create

b)collaberative filtering - recommands content on the basis of user similarity/user interest eg. If A and B are 2 person and both 
have similar choice of things. if both have watch similar movie for eg. 3idiots and both like them or give 
similar rating and if user A watch some another movie lets say M2 and give good rating then that movie also get
recommended to the User B whose have same similar choice.    

c)hybrid - whose base on collab of both a and b types. youtube have this type in their recommedation

project::::
1)column of overview is in form of string so we are convert it into the list so we can concatenate with 
other list.



---similarity of movies can be recognize on the basis of tags 

***TEXT VECTORIZATION
--we have data in form of -- movie id,name,tags
--we have to convert tags into vectors it has different techniques(TFIDF, WORD2VEC) 
apart that we are gonna use BAG OF WORDS
--In bag of words we combine all tags from movie and from that the word which is frequently used/most common word
appear in that (5000 most common word) we can extact that word from that text then from tags we will ask
how many times a common word appear in 1st movie same for 2nd movie and so on.....upto 5000 movie 
---while doing VECTORIZATIOn we are not gonna consider stop words. stop words are those word which are used in 
sentance formation but don't have any contibution in mening of senatnce like - a,is,and,or,of,to,from
---for this vectorization we are gonna use sklitlearn(sklearn) library and inside that we are goona use 
countvectorize function 
--max feature will tell how many word u want to take, stop word
---countvector obj return scipy spars martix so we convert it into numpy array
---stem is done to stop repeation of word also called as stemming

---we have to calculate distance of each movie with each another if distance is more similarity is less
we are not gonna calculate Euclidean distance(distance betn 2 points/tip to tip distance formula ------
under root(x2-x1)^2 + (y2-y1)^2) it is not a good measure for high no. of data
----instead of that we will calculate angle between them by cosine distance(theta between 2 vectors)
if theta is low the movies are similar to each other(distance is inversly proportinal to similarity)
we we calculate this by using sklitlearn library with cosine similarity function
